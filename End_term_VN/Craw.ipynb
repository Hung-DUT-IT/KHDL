{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vietnamese Emotion Classification using PhoBERT\n",
    "- Input:\n",
    "    - Sentence\n",
    "- Output:\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python3.9\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import csv\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import plotly.express as px\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, recall_score, f1_score, make_scorer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stop_words = set(stopwords.words('vietnamese')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained PhoBERT tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
    "model = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "encode_lable = LabelEncoder()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Craw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(name, a):\n",
    "    df = pd.DataFrame.from_dict(a, orient='index')\n",
    "    df = df.transpose()\n",
    "    dataset = pd.DataFrame(data=df)\n",
    "    dataset.to_excel(f'./DataSet/{name}.xlsx', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(name, dict_url):\n",
    "\n",
    "    driver = webdriver.Edge(\"./msedgedriver.exe\")\n",
    "\n",
    "    Contents = []\n",
    "    Topic = []\n",
    "    dictionary = {'Content': Contents, 'Topic': Topic}\n",
    "\n",
    "    for _topic, quantity in dict_url.items():\n",
    "        driver.get(f'https://vtv.vn/{_topic}.htm')\n",
    "        content = []\n",
    "        count = 0\n",
    "        status = True\n",
    "        while status:\n",
    "            items = driver.find_elements(By.XPATH, '/html/body/form/div[2]/div[3]/div[3]/div/div[1]/div[1]/div[3]/div[1]/ul/li')\n",
    "\n",
    "            for item in items:\n",
    "                if count >= quantity:\n",
    "                    status = False\n",
    "                    break\n",
    "                # Tìm tiêu đề tin tức\n",
    "                content.append(item.find_element(By.XPATH,  './/h4/a').text)\n",
    "                topic = [_topic for _ in range(len(content))]\n",
    "                count += 1\n",
    "            # Tìm nút \"Xem thêm\"\n",
    "            load_more_button = driver.find_element(By.XPATH, '/html/body/form/div[2]/div[3]/div[3]/div/div[1]/div[1]/div[3]/div[2]/a')\n",
    "\n",
    "            if not load_more_button.is_displayed():\n",
    "                break\n",
    "            load_more_button.click()\n",
    "\n",
    "            time.sleep(5)\n",
    "        Topic.extend(topic)\n",
    "        Contents.extend(content)\n",
    "    save(name, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_url_big = {    'chinh-tri':        1856,\n",
    "                    'xa-hoi':           1005,\n",
    "                    'kinh-te':          2326,\n",
    "                    'truyen-hinh':      2164,\n",
    "                    'cong-nghe':        548,\n",
    "                    'doi-song':         1456,\n",
    "                    'van-hoa-giai-tri': 1564\n",
    "                }\n",
    "\n",
    "dict_url_small = {  'chinh-tri':        213,\n",
    "                    'xa-hoi':           113,\n",
    "                    'kinh-te':          566,\n",
    "                    'truyen-hinh':      36,\n",
    "                    'cong-nghe':        156,\n",
    "                    'doi-song':         102,\n",
    "                    'van-hoa-giai-tri': 189\n",
    "                }\n",
    "\n",
    "# get_data(\"big_data\", dict_url_big)\n",
    "# get_data(\"small_data\", dict_url_small)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    df = pd.read_excel(path, sheet_name=None)['Sheet1']\n",
    "    df.columns = ['index', 'Content', 'Topic']\n",
    "    # unused column\n",
    "    df.drop(columns=['index'], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small = read_data('./DataSet/small_data.xlsx')\n",
    "df_big = read_data('./DataSet/big_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Quán triệt các quy định của Ban Bí thư về báo ...</td>\n",
       "      <td>chinh-tri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Phó Thủ tướng Trần Hồng Hà: Không để thiếu vac...</td>\n",
       "      <td>chinh-tri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ASEAN cần thích ứng năng động và tăng cường sứ...</td>\n",
       "      <td>chinh-tri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tiếp tục hoàn thiện cơ chế xác định giá đất</td>\n",
       "      <td>chinh-tri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bộ Công an trao tặng Giải thưởng Trần Quốc Hoà...</td>\n",
       "      <td>chinh-tri</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content      Topic\n",
       "0  Quán triệt các quy định của Ban Bí thư về báo ...  chinh-tri\n",
       "1  Phó Thủ tướng Trần Hồng Hà: Không để thiếu vac...  chinh-tri\n",
       "2  ASEAN cần thích ứng năng động và tăng cường sứ...  chinh-tri\n",
       "3        Tiếp tục hoàn thiện cơ chế xác định giá đất  chinh-tri\n",
       "4  Bộ Công an trao tặng Giải thưởng Trần Quốc Hoà...  chinh-tri"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thủ tướng Phạm Minh Chính tiếp xúc cử tri trướ...</td>\n",
       "      <td>chinh-tri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thủ tướng bổ nhiệm lại Thứ trưởng Bộ Công Thương</td>\n",
       "      <td>chinh-tri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thủ tướng Phạm Minh Chính dự khai mạc Lễ hội H...</td>\n",
       "      <td>chinh-tri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“Không có vướng mắc nào của địa phương không đ...</td>\n",
       "      <td>chinh-tri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dù ở vị trí nào mà có trách nhiệm, khát khao c...</td>\n",
       "      <td>chinh-tri</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content      Topic\n",
       "0  Thủ tướng Phạm Minh Chính tiếp xúc cử tri trướ...  chinh-tri\n",
       "1   Thủ tướng bổ nhiệm lại Thứ trưởng Bộ Công Thương  chinh-tri\n",
       "2  Thủ tướng Phạm Minh Chính dự khai mạc Lễ hội H...  chinh-tri\n",
       "3  “Không có vướng mắc nào của địa phương không đ...  chinh-tri\n",
       "4  Dù ở vị trí nào mà có trách nhiệm, khát khao c...  chinh-tri"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_big.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_topic(df):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    sns.countplot(x='Topic', data=df)\n",
    "    plt.title(\"Number of sample\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_topic(df_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_topic(df_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_big['Topic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_small['Topic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_topic(topic):\n",
    "    if topic in ['chinh-tri']:\n",
    "        return 'chinh-tri'\n",
    "    elif (topic in ['kinh-te']):\n",
    "        return 'kinh-te'\n",
    "    elif (topic in ['cong-nghe']):\n",
    "        return 'Công-nghệ'\n",
    "    elif (topic in ['xa-hoi']):\n",
    "        return 'xa-hoi'\n",
    "    else: \n",
    "        return 'Khác'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small['Topic'] = df_small['Topic'].apply(lambda topic: group_topic(topic))\n",
    "df_big['Topic'] = df_big['Topic'].apply(lambda topic: group_topic(topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_small['Topic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_topic(df_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_big['Topic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_topic(df_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_count_token():\n",
    "    all_data = df_big['Content'].tolist() + df_small['Content'].tolist()\n",
    "    token_text = [tokenizer.encode(text, add_special_tokens=True) for text in all_data]\n",
    "    token_lens = [len(text) for text in token_text]\n",
    "\n",
    "    sns.set(rc={'figure.figsize':(20, 10)})\n",
    "    sns.displot(token_lens, height=5, aspect=3)\n",
    "    plt.xlim([0,max(token_lens)])\n",
    "    plt.xlabel('Token Count')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_count_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_word():\n",
    "    top = Counter([item for sublist in df_big['Content'].apply(lambda x:str(x).split()) for item in sublist])\n",
    "    print(len(top))\n",
    "    temp = pd.DataFrame(top.most_common(50))\n",
    "    temp.columns = ['Common_words','count']\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3334\n"
     ]
    }
   ],
   "source": [
    "top_word = top_word()\n",
    "#top_word.style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fig = px.bar(top_word, x=\"count\", y=\"Common_words\", title='Commmon Words in Contents', orientation='h', \n",
    "#              width=700, height=700,color='Common_words')\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plt.subplots(figsize=(20, 10))\n",
    "\n",
    "# wordcloud = WordCloud (\n",
    "#                     background_color = 'white',\n",
    "#                     width = 512,\n",
    "#                     height = 384\n",
    "#                         ).generate(' '.join(top_word['Common_words']))\n",
    "# plt.imshow(wordcloud) # image show\n",
    "# plt.axis('off') # to off the axis of x and y\n",
    "# plt.savefig('Plotly-World_Cloud.png')\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and Processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(content):\n",
    "    text = ' '.join(word for word in content.split() if word not in stop_words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removing_numbers(content):\n",
    "    content = ''.join([i for i in content if not i.isdigit()])\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(content):\n",
    "    content = content.split()\n",
    "    content = [y.lower() for y in content]\n",
    "    return \" \" .join(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Removing_punctuations(content):\n",
    "    ## Remove punctuations\n",
    "    content = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,،-./:;<=>؟?@[\\]^_`{|}~\"\"\"), ' ', content)\n",
    "    content = content.replace('؛',\"\", )\n",
    "    \n",
    "    ## remove extra whitespace\n",
    "    content = re.sub('\\s+', ' ', content)\n",
    "    content =  \" \".join(content.split())\n",
    "    return content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_sentence(content):\n",
    "    content = lower_case(content)\n",
    "    content = Removing_punctuations(content)\n",
    "    content = remove_stop_words(content)\n",
    "    content = removing_numbers(content)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(df):\n",
    "    df['Content'] = df['Content'].apply(lambda content : lower_case(content))\n",
    "    df['Content'] = df['Content'].apply(lambda content : remove_stop_words(content))\n",
    "    df['Content'] = df['Content'].apply(lambda content : Removing_punctuations(content))\n",
    "    df['Content'] = df['Content'].apply(lambda content : removing_numbers(content))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_small.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_big.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small = normalize_text(df_small)\n",
    "df_big = normalize_text(df_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_small.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top = Counter([item for sublist in df_big['Content'].apply(lambda x:str(x).split()) for item in sublist])\n",
    "# print(len(top))\n",
    "# temp = pd.DataFrame(top.most_common(20))\n",
    "# temp.columns = ['Common_words','count']\n",
    "# temp.style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_count_token()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract feature -> endcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(64001, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(258, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to generate sentence embeddings\n",
    "def generate_sentence_embedding(text):\n",
    "    # Tokenize the text\n",
    "    input_ids = torch.tensor([tokenizer.encode(text, add_special_tokens=True)])\n",
    "    # Generate the embedding\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_lables(df):\n",
    "    encode_lable.fit(df['Topic'])\n",
    "    df['Topic'] = encode_lable.transform(df['Topic'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_sentences(df):\n",
    "    df_t = df.copy()\n",
    "    df_t['Content_embedding'] = df_t['Content'].apply(lambda content : generate_sentence_embedding(content))\n",
    "    return df_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_sentences_Norm(df):\n",
    "    df_t = df.copy()\n",
    "    norm = Normalizer()\n",
    "    df_t['Content_embedding'] = df_t['Content'].apply(lambda content : generate_sentence_embedding(content))\n",
    "    df_t['Content_embedding'] = df_t['Content_embedding'].apply(lambda x: norm.fit_transform([x])[0])\n",
    "    return df_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode_lables(df_small)\n",
    "# embedding_sentences(df_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_sentences_Norm(df_small)\n",
    "# df_small.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode_lables(df_big)\n",
    "# embedding_sentences(df_big)\n",
    "# Norm(df_big)\n",
    "# df_big.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_small['Content_embedding'][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_content_3d(df):\n",
    "    X = np.array(df['Content_embedding'].tolist())\n",
    "    y = np.array(df['Topic'].tolist())\n",
    "\n",
    "    # Sử dụng t-SNE để giảm chiều dữ liệu từ không gian đa chiều xuống không gian 3D\n",
    "    tsne = TSNE(n_components=3)\n",
    "    embeddings = tsne.fit_transform(X)\n",
    "\n",
    "    # Tạo biểu đồ 3D và phân biệt các lớp\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Với mỗi lớp, tô màu các điểm tương ứng trên biểu đồ\n",
    "    for label in np.unique(y):\n",
    "        indices = np.where(y == label)\n",
    "        ax.scatter(embeddings[indices, 0], embeddings[indices, 1], embeddings[indices, 2], label=label)\n",
    "\n",
    "    # Thêm chú thích cho các lớp\n",
    "    ax.legend()\n",
    "\n",
    "    # Hiển thị biểu đồ\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_content_2d(df):\n",
    "    X = np.array(df['Content_embedding'].tolist())\n",
    "    y = np.array(df['Topic'].tolist())\n",
    "\n",
    "    # Sử dụng t-SNE để giảm chiều dữ liệu từ không gian đa chiều xuống không gian 2D\n",
    "    tsne = TSNE(n_components=2)\n",
    "    embeddings = tsne.fit_transform(X)\n",
    "\n",
    "    # Tạo biểu đồ 2D và phân biệt các lớp\n",
    "    plt.figure(figsize=(20, 10))\n",
    "\n",
    "    # Với mỗi lớp, tô màu các điểm tương ứng trên biểu đồ\n",
    "    for label in np.unique(y):\n",
    "        indices = np.where(y == label)\n",
    "        plt.scatter(embeddings[indices, 0], embeddings[indices, 1], label=label)\n",
    "\n",
    "    # Thêm chú thích cho các lớp\n",
    "    plt.legend()\n",
    "\n",
    "    # Hiển thị biểu đồ\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "##plot_content_2d(df_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "##plot_content_2d(df_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_content_3d(df_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_content_3d(df_big)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_F1(trained_model,X,y):\n",
    "\n",
    "    predicted = trained_model.predict(X)\n",
    "    # Calculate the F1 score for the predictions\n",
    "    f1 = f1_score(y, predicted, average=None)\n",
    "    # Return the F1 score\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eveluation_model(model, X_test, y_test):\n",
    "    \n",
    "    #test the model with the test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    #calculate the accuracy\n",
    "    log_reg_accuracy = accuracy_score(y_test, y_pred)\n",
    "    print('Accuracy: ', log_reg_accuracy,'\\n')\n",
    "\n",
    "    #calculate the F1 score\n",
    "    f1_Score = get_F1(model,X_test,y_test)\n",
    "    return pd.DataFrame(f1_Score, index= df_small['Topic'].unique(), columns=['F1 score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model(model, df):\n",
    "    X = df[\"Content_embedding\"].values\n",
    "    X = np.vstack(X)\n",
    "    y = df[\"Topic\"].values\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X , y, test_size = 0.3, random_state = 6, stratify = y)\n",
    "    # cross_val_score(classifier, X_train, y_train, scoring = 'accuracy', cv =10).mean()\n",
    "    model.fit(X_train, y_train)\n",
    "    eveluation = eveluation_model(model, X_test ,y_test)\n",
    "    return eveluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model_repeat_holdout(model, df):\n",
    "    X = df[\"Content_embedding\"].values\n",
    "    X = np.vstack(X)\n",
    "    y = df[\"Topic\"].values\n",
    "    acc = []\n",
    "    f1 = []\n",
    "    recall = []\n",
    "    for i in range(10):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=i,stratify=y)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        acc.append(accuracy_score(y_test, y_pred))\n",
    "        f1.append(f1_score(y_test, y_pred, average='macro'))\n",
    "        recall.append(recall_score(y_test, y_pred, average='macro'))\n",
    "    return np.mean(acc), np.mean(recall), np.mean(f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model_cross_val(model, df):\n",
    "    scoringopt = {'accuracy' : make_scorer(accuracy_score),\n",
    "              'recall': make_scorer(recall_score, average='macro'),\n",
    "              'f1_score': make_scorer(f1_score, average='macro')}\n",
    "    list = []\n",
    "    X = df[\"Content_embedding\"].values\n",
    "    X = np.vstack(X)\n",
    "    y = df[\"Topic\"].values\n",
    "    cv_results = cross_validate(model, X, y, cv=5, scoring=scoringopt)\n",
    "    return np.mean(cv_results['test_accuracy']), np.mean(cv_results['test_recall']), np.mean(cv_results['test_f1_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Normalize</th>\n",
       "      <th>Model</th>\n",
       "      <th>evaluation</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DataFrame</td>\n",
       "      <td>embedding_sentences</td>\n",
       "      <td>SVC</td>\n",
       "      <td>training_model_repeat_holdout</td>\n",
       "      <td>0.938257</td>\n",
       "      <td>0.918937</td>\n",
       "      <td>0.925011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DataFrame</td>\n",
       "      <td>embedding_sentences</td>\n",
       "      <td>SVC</td>\n",
       "      <td>training_model_cross_val</td>\n",
       "      <td>0.944727</td>\n",
       "      <td>0.924442</td>\n",
       "      <td>0.928273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DataFrame</td>\n",
       "      <td>embedding_sentences</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>training_model_repeat_holdout</td>\n",
       "      <td>0.935109</td>\n",
       "      <td>0.904388</td>\n",
       "      <td>0.922613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DataFrame</td>\n",
       "      <td>embedding_sentences</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>training_model_cross_val</td>\n",
       "      <td>0.952000</td>\n",
       "      <td>0.926520</td>\n",
       "      <td>0.943280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DataFrame</td>\n",
       "      <td>embedding_sentences_Norm</td>\n",
       "      <td>SVC</td>\n",
       "      <td>training_model_repeat_holdout</td>\n",
       "      <td>0.844552</td>\n",
       "      <td>0.767694</td>\n",
       "      <td>0.798750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DataFrame</td>\n",
       "      <td>embedding_sentences_Norm</td>\n",
       "      <td>SVC</td>\n",
       "      <td>training_model_cross_val</td>\n",
       "      <td>0.864727</td>\n",
       "      <td>0.799657</td>\n",
       "      <td>0.829067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DataFrame</td>\n",
       "      <td>embedding_sentences_Norm</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>training_model_repeat_holdout</td>\n",
       "      <td>0.929782</td>\n",
       "      <td>0.899610</td>\n",
       "      <td>0.917705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DataFrame</td>\n",
       "      <td>embedding_sentences_Norm</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>training_model_cross_val</td>\n",
       "      <td>0.947636</td>\n",
       "      <td>0.923687</td>\n",
       "      <td>0.939737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Dataset                 Normalize                   Model  \\\n",
       "0  DataFrame       embedding_sentences                     SVC   \n",
       "1  DataFrame       embedding_sentences                     SVC   \n",
       "2  DataFrame       embedding_sentences  RandomForestClassifier   \n",
       "3  DataFrame       embedding_sentences  RandomForestClassifier   \n",
       "4  DataFrame  embedding_sentences_Norm                     SVC   \n",
       "5  DataFrame  embedding_sentences_Norm                     SVC   \n",
       "6  DataFrame  embedding_sentences_Norm  RandomForestClassifier   \n",
       "7  DataFrame  embedding_sentences_Norm  RandomForestClassifier   \n",
       "\n",
       "                      evaluation  accuracy    recall        f1  \n",
       "0  training_model_repeat_holdout  0.938257  0.918937  0.925011  \n",
       "1       training_model_cross_val  0.944727  0.924442  0.928273  \n",
       "2  training_model_repeat_holdout  0.935109  0.904388  0.922613  \n",
       "3       training_model_cross_val  0.952000  0.926520  0.943280  \n",
       "4  training_model_repeat_holdout  0.844552  0.767694  0.798750  \n",
       "5       training_model_cross_val  0.864727  0.799657  0.829067  \n",
       "6  training_model_repeat_holdout  0.929782  0.899610  0.917705  \n",
       "7       training_model_cross_val  0.947636  0.923687  0.939737  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_SVC = SVC(kernel='linear', probability=True)\n",
    "model_RandomForest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "data = [df_small]\n",
    "norm = [embedding_sentences, embedding_sentences_Norm]\n",
    "md = [model_SVC, model_RandomForest]\n",
    "evaluation = [training_model_repeat_holdout, training_model_cross_val]\n",
    "encode_lables(df_small)\n",
    "encode_lables(df_big)\n",
    "result_list = []\n",
    "for i in data:\n",
    "    for j in norm:\n",
    "        df_t = j(i)\n",
    "        for k in md:\n",
    "            for l in evaluation:\n",
    "                acc, recall, f1 = l(k,df_t)\n",
    "                result_dict = {\n",
    "                'Dataset':  type(i).__name__,\n",
    "                'Normalize': j.__name__,\n",
    "                'Model': type(k).__name__,\n",
    "                'evaluation' : l.__name__,\n",
    "                'accuracy' : acc,\n",
    "                'recall' : recall,\n",
    "                'f1' : f1\n",
    "                }\n",
    "                result_list.append(result_dict)\n",
    "rs = pd.DataFrame(result_list)\n",
    "rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_model_cross_val(SVC(kernel='linear', probability=True), df_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_model(SVC(kernel='linear', probability=True), df_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_model_cross_val(RandomForestClassifier(n_estimators=100, random_state=42), df_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_model(RandomForestClassifier(n_estimators=100, random_state=42), df_big)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0rc2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
